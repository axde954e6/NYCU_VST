{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VSAT_HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lrPNw4LRqR1"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su7-rKOron8D"
      },
      "source": [
        "import numpy as np\n",
        "x_valid = np.load('/content/drive/MyDrive/VSAT_HW1/x_valid.npy')\n",
        "x_train = np.load('/content/drive/MyDrive/VSAT_HW1/x_train.npy')\n",
        "y_train=np.load('/content/drive/MyDrive/VSAT_HW1/y_train.npy')\n",
        "y_valid=np.load('/content/drive/MyDrive/VSAT_HW1/y_valid.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcQvSjUrmb8H"
      },
      "source": [
        "def convert_CHW(data):\n",
        "  transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "  tmp=[]\n",
        "  for i,img in enumerate(data,0):\n",
        "    image=np.array(transform(img).tolist())\n",
        "    tmp.append(image)\n",
        "  tmp=np.array(tmp)\n",
        "  return tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KN6cz-1mi1M"
      },
      "source": [
        "#transform\n",
        "x_train=convert_CHW(x_train)\n",
        "x_valid=convert_CHW(x_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSCAEkSXQrNq"
      },
      "source": [
        "import torch \n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self,data,label,transform):\n",
        "        self.data = torch.FloatTensor(data)\n",
        "        self.label = torch.LongTensor(label)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        if self.transform:\n",
        "            self.data[index] = self.data[index]\n",
        "        return self.data[index],self.label[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDCq7cuMU0s0",
        "outputId": "1ea01265-ab87-4e87-99f6-9db307c9be18"
      },
      "source": [
        "# GPU\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print('GPU state:', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU state: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya-EYpsEU2fz"
      },
      "source": [
        "#dataset\n",
        "trainset=MyDataset(data=x_train,label=y_train,transform=None)\n",
        "testset=MyDataset(data=x_valid,label=y_valid,transform=None)\n",
        "#dataLoader\n",
        "trainLoader = DataLoader(dataset=trainset,batch_size=8,shuffle=True,num_workers=2)\n",
        "testLoader = DataLoader(dataset=testset,batch_size=1,num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN1gBF0B3NnA",
        "outputId": "a9c395c1-3ce5-4e6e-cdf4-6c25bf6d25bd"
      },
      "source": [
        "# Model structure\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsEn85YVBL7"
      },
      "source": [
        "# Parameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "epochs = 8\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW43Fo0Lt_C_"
      },
      "source": [
        "def cal_accuracy(net,Loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  loss=0\n",
        "  running_loss,times=0,0\n",
        "  with torch.no_grad():\n",
        "      for data in Loader:\n",
        "          inputs, labels = data\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          values, predicted = outputs.topk(3, dim=1, largest=True, sorted=True)\n",
        "          total += labels.size(0)\n",
        "          for i,data in enumerate(labels,0):\n",
        "            if data in predicted[i]:\n",
        "              correct+=1\n",
        "          running_loss+=loss.item()\n",
        "          times+=1\n",
        "  return correct/total*100,running_loss/(times)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PLU5z-QVFib",
        "outputId": "656670f7-5487-4a83-9be0-3b039b1cd5d8"
      },
      "source": [
        "# Train\n",
        "train_loss_value=[]\n",
        "valid_loss_value=[]\n",
        "now_epoch=0\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for times, data in enumerate(trainLoader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    now_valid_accuracy,now_valid_loss=cal_accuracy(net,testLoader)\n",
        "    tmp,now_train_loss=cal_accuracy(net,trainLoader)\n",
        "    print('[%d/%d, %d] train loss: %.3f validation loss: %.3f validation accuracy: %.1f' % (epoch+1, epochs, len(trainLoader), now_train_loss,now_valid_loss,now_valid_accuracy))\n",
        "    train_loss_value.append(running_loss/(times+1))\n",
        "    valid_loss_value.append(now_valid_loss)\n",
        "    \n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/8, 100/5000] loss: 0.115\n",
            "[1/8, 200/5000] loss: 0.230\n",
            "[1/8, 300/5000] loss: 0.345\n",
            "[1/8, 400/5000] loss: 0.460\n",
            "[1/8, 500/5000] loss: 0.575\n",
            "[1/8, 600/5000] loss: 0.690\n",
            "[1/8, 700/5000] loss: 0.804\n",
            "[1/8, 800/5000] loss: 0.918\n",
            "[1/8, 900/5000] loss: 1.031\n",
            "[1/8, 1000/5000] loss: 1.142\n",
            "[1/8, 1100/5000] loss: 1.253\n",
            "[1/8, 1200/5000] loss: 1.363\n",
            "[1/8, 1300/5000] loss: 1.472\n",
            "[1/8, 1400/5000] loss: 1.579\n",
            "[1/8, 1500/5000] loss: 1.685\n",
            "[1/8, 1600/5000] loss: 1.790\n",
            "[1/8, 1700/5000] loss: 1.895\n",
            "[1/8, 1800/5000] loss: 1.998\n",
            "[1/8, 1900/5000] loss: 2.101\n",
            "[1/8, 2000/5000] loss: 2.201\n",
            "[1/8, 2100/5000] loss: 2.301\n",
            "[1/8, 2200/5000] loss: 2.401\n",
            "[1/8, 2300/5000] loss: 2.497\n",
            "[1/8, 2400/5000] loss: 2.596\n",
            "[1/8, 2500/5000] loss: 2.690\n",
            "[1/8, 2600/5000] loss: 2.786\n",
            "[1/8, 2700/5000] loss: 2.880\n",
            "[1/8, 2800/5000] loss: 2.974\n",
            "[1/8, 2900/5000] loss: 3.071\n",
            "[1/8, 3000/5000] loss: 3.165\n",
            "[1/8, 3100/5000] loss: 3.260\n",
            "[1/8, 3200/5000] loss: 3.352\n",
            "[1/8, 3300/5000] loss: 3.445\n",
            "[1/8, 3400/5000] loss: 3.536\n",
            "[1/8, 3500/5000] loss: 3.624\n",
            "[1/8, 3600/5000] loss: 3.716\n",
            "[1/8, 3700/5000] loss: 3.803\n",
            "[1/8, 3800/5000] loss: 3.892\n",
            "[1/8, 3900/5000] loss: 3.976\n",
            "[1/8, 4000/5000] loss: 4.063\n",
            "[1/8, 4100/5000] loss: 4.149\n",
            "[1/8, 4200/5000] loss: 4.237\n",
            "[1/8, 4300/5000] loss: 4.322\n",
            "[1/8, 4400/5000] loss: 4.406\n",
            "[1/8, 4500/5000] loss: 4.492\n",
            "[1/8, 4600/5000] loss: 4.575\n",
            "[1/8, 4700/5000] loss: 4.657\n",
            "[1/8, 4800/5000] loss: 4.739\n",
            "[1/8, 4900/5000] loss: 4.822\n",
            "[1/8, 5000/5000] loss: 4.904\n",
            "[2/8, 100/5000] loss: 0.081\n",
            "[2/8, 200/5000] loss: 0.161\n",
            "[2/8, 300/5000] loss: 0.242\n",
            "[2/8, 400/5000] loss: 0.322\n",
            "[2/8, 500/5000] loss: 0.403\n",
            "[2/8, 600/5000] loss: 0.482\n",
            "[2/8, 700/5000] loss: 0.561\n",
            "[2/8, 800/5000] loss: 0.644\n",
            "[2/8, 900/5000] loss: 0.723\n",
            "[2/8, 1000/5000] loss: 0.802\n",
            "[2/8, 1100/5000] loss: 0.881\n",
            "[2/8, 1200/5000] loss: 0.959\n",
            "[2/8, 1300/5000] loss: 1.039\n",
            "[2/8, 1400/5000] loss: 1.113\n",
            "[2/8, 1500/5000] loss: 1.190\n",
            "[2/8, 1600/5000] loss: 1.266\n",
            "[2/8, 1700/5000] loss: 1.341\n",
            "[2/8, 1800/5000] loss: 1.418\n",
            "[2/8, 1900/5000] loss: 1.494\n",
            "[2/8, 2000/5000] loss: 1.569\n",
            "[2/8, 2100/5000] loss: 1.645\n",
            "[2/8, 2200/5000] loss: 1.718\n",
            "[2/8, 2300/5000] loss: 1.794\n",
            "[2/8, 2400/5000] loss: 1.869\n",
            "[2/8, 2500/5000] loss: 1.946\n",
            "[2/8, 2600/5000] loss: 2.021\n",
            "[2/8, 2700/5000] loss: 2.096\n",
            "[2/8, 2800/5000] loss: 2.172\n",
            "[2/8, 2900/5000] loss: 2.247\n",
            "[2/8, 3000/5000] loss: 2.320\n",
            "[2/8, 3100/5000] loss: 2.395\n",
            "[2/8, 3200/5000] loss: 2.465\n",
            "[2/8, 3300/5000] loss: 2.539\n",
            "[2/8, 3400/5000] loss: 2.612\n",
            "[2/8, 3500/5000] loss: 2.686\n",
            "[2/8, 3600/5000] loss: 2.757\n",
            "[2/8, 3700/5000] loss: 2.831\n",
            "[2/8, 3800/5000] loss: 2.901\n",
            "[2/8, 3900/5000] loss: 2.977\n",
            "[2/8, 4000/5000] loss: 3.048\n",
            "[2/8, 4100/5000] loss: 3.120\n",
            "[2/8, 4200/5000] loss: 3.191\n",
            "[2/8, 4300/5000] loss: 3.264\n",
            "[2/8, 4400/5000] loss: 3.336\n",
            "[2/8, 4500/5000] loss: 3.404\n",
            "[2/8, 4600/5000] loss: 3.477\n",
            "[2/8, 4700/5000] loss: 3.545\n",
            "[2/8, 4800/5000] loss: 3.617\n",
            "[2/8, 4900/5000] loss: 3.687\n",
            "[2/8, 5000/5000] loss: 3.761\n",
            "[3/8, 100/5000] loss: 0.067\n",
            "[3/8, 200/5000] loss: 0.137\n",
            "[3/8, 300/5000] loss: 0.203\n",
            "[3/8, 400/5000] loss: 0.272\n",
            "[3/8, 500/5000] loss: 0.342\n",
            "[3/8, 600/5000] loss: 0.412\n",
            "[3/8, 700/5000] loss: 0.478\n",
            "[3/8, 800/5000] loss: 0.546\n",
            "[3/8, 900/5000] loss: 0.612\n",
            "[3/8, 1000/5000] loss: 0.683\n",
            "[3/8, 1100/5000] loss: 0.752\n",
            "[3/8, 1200/5000] loss: 0.822\n",
            "[3/8, 1300/5000] loss: 0.889\n",
            "[3/8, 1400/5000] loss: 0.955\n",
            "[3/8, 1500/5000] loss: 1.020\n",
            "[3/8, 1600/5000] loss: 1.087\n",
            "[3/8, 1700/5000] loss: 1.155\n",
            "[3/8, 1800/5000] loss: 1.222\n",
            "[3/8, 1900/5000] loss: 1.287\n",
            "[3/8, 2000/5000] loss: 1.353\n",
            "[3/8, 2100/5000] loss: 1.419\n",
            "[3/8, 2200/5000] loss: 1.487\n",
            "[3/8, 2300/5000] loss: 1.554\n",
            "[3/8, 2400/5000] loss: 1.623\n",
            "[3/8, 2500/5000] loss: 1.689\n",
            "[3/8, 2600/5000] loss: 1.755\n",
            "[3/8, 2700/5000] loss: 1.821\n",
            "[3/8, 2800/5000] loss: 1.888\n",
            "[3/8, 2900/5000] loss: 1.954\n",
            "[3/8, 3000/5000] loss: 2.024\n",
            "[3/8, 3100/5000] loss: 2.093\n",
            "[3/8, 3200/5000] loss: 2.161\n",
            "[3/8, 3300/5000] loss: 2.228\n",
            "[3/8, 3400/5000] loss: 2.297\n",
            "[3/8, 3500/5000] loss: 2.361\n",
            "[3/8, 3600/5000] loss: 2.428\n",
            "[3/8, 3700/5000] loss: 2.495\n",
            "[3/8, 3800/5000] loss: 2.562\n",
            "[3/8, 3900/5000] loss: 2.627\n",
            "[3/8, 4000/5000] loss: 2.693\n",
            "[3/8, 4100/5000] loss: 2.760\n",
            "[3/8, 4200/5000] loss: 2.826\n",
            "[3/8, 4300/5000] loss: 2.894\n",
            "[3/8, 4400/5000] loss: 2.959\n",
            "[3/8, 4500/5000] loss: 3.027\n",
            "[3/8, 4600/5000] loss: 3.094\n",
            "[3/8, 4700/5000] loss: 3.158\n",
            "[3/8, 4800/5000] loss: 3.225\n",
            "[3/8, 4900/5000] loss: 3.286\n",
            "[3/8, 5000/5000] loss: 3.353\n",
            "[4/8, 100/5000] loss: 0.060\n",
            "[4/8, 200/5000] loss: 0.123\n",
            "[4/8, 300/5000] loss: 0.189\n",
            "[4/8, 400/5000] loss: 0.253\n",
            "[4/8, 500/5000] loss: 0.316\n",
            "[4/8, 600/5000] loss: 0.377\n",
            "[4/8, 700/5000] loss: 0.436\n",
            "[4/8, 800/5000] loss: 0.498\n",
            "[4/8, 900/5000] loss: 0.562\n",
            "[4/8, 1000/5000] loss: 0.625\n",
            "[4/8, 1100/5000] loss: 0.687\n",
            "[4/8, 1200/5000] loss: 0.749\n",
            "[4/8, 1300/5000] loss: 0.811\n",
            "[4/8, 1400/5000] loss: 0.872\n",
            "[4/8, 1500/5000] loss: 0.934\n",
            "[4/8, 1600/5000] loss: 0.999\n",
            "[4/8, 1700/5000] loss: 1.059\n",
            "[4/8, 1800/5000] loss: 1.120\n",
            "[4/8, 1900/5000] loss: 1.187\n",
            "[4/8, 2000/5000] loss: 1.252\n",
            "[4/8, 2100/5000] loss: 1.315\n",
            "[4/8, 2200/5000] loss: 1.374\n",
            "[4/8, 2300/5000] loss: 1.439\n",
            "[4/8, 2400/5000] loss: 1.502\n",
            "[4/8, 2500/5000] loss: 1.564\n",
            "[4/8, 2600/5000] loss: 1.628\n",
            "[4/8, 2700/5000] loss: 1.691\n",
            "[4/8, 2800/5000] loss: 1.751\n",
            "[4/8, 2900/5000] loss: 1.813\n",
            "[4/8, 3000/5000] loss: 1.877\n",
            "[4/8, 3100/5000] loss: 1.940\n",
            "[4/8, 3200/5000] loss: 2.004\n",
            "[4/8, 3300/5000] loss: 2.069\n",
            "[4/8, 3400/5000] loss: 2.132\n",
            "[4/8, 3500/5000] loss: 2.194\n",
            "[4/8, 3600/5000] loss: 2.255\n",
            "[4/8, 3700/5000] loss: 2.315\n",
            "[4/8, 3800/5000] loss: 2.376\n",
            "[4/8, 3900/5000] loss: 2.439\n",
            "[4/8, 4000/5000] loss: 2.499\n",
            "[4/8, 4100/5000] loss: 2.560\n",
            "[4/8, 4200/5000] loss: 2.617\n",
            "[4/8, 4300/5000] loss: 2.679\n",
            "[4/8, 4400/5000] loss: 2.741\n",
            "[4/8, 4500/5000] loss: 2.800\n",
            "[4/8, 4600/5000] loss: 2.861\n",
            "[4/8, 4700/5000] loss: 2.922\n",
            "[4/8, 4800/5000] loss: 2.983\n",
            "[4/8, 4900/5000] loss: 3.046\n",
            "[4/8, 5000/5000] loss: 3.105\n",
            "[5/8, 100/5000] loss: 0.061\n",
            "[5/8, 200/5000] loss: 0.121\n",
            "[5/8, 300/5000] loss: 0.178\n",
            "[5/8, 400/5000] loss: 0.237\n",
            "[5/8, 500/5000] loss: 0.293\n",
            "[5/8, 600/5000] loss: 0.350\n",
            "[5/8, 700/5000] loss: 0.409\n",
            "[5/8, 800/5000] loss: 0.467\n",
            "[5/8, 900/5000] loss: 0.523\n",
            "[5/8, 1000/5000] loss: 0.582\n",
            "[5/8, 1100/5000] loss: 0.645\n",
            "[5/8, 1200/5000] loss: 0.702\n",
            "[5/8, 1300/5000] loss: 0.761\n",
            "[5/8, 1400/5000] loss: 0.821\n",
            "[5/8, 1500/5000] loss: 0.879\n",
            "[5/8, 1600/5000] loss: 0.940\n",
            "[5/8, 1700/5000] loss: 0.999\n",
            "[5/8, 1800/5000] loss: 1.059\n",
            "[5/8, 1900/5000] loss: 1.121\n",
            "[5/8, 2000/5000] loss: 1.175\n",
            "[5/8, 2100/5000] loss: 1.230\n",
            "[5/8, 2200/5000] loss: 1.287\n",
            "[5/8, 2300/5000] loss: 1.345\n",
            "[5/8, 2400/5000] loss: 1.405\n",
            "[5/8, 2500/5000] loss: 1.460\n",
            "[5/8, 2600/5000] loss: 1.516\n",
            "[5/8, 2700/5000] loss: 1.574\n",
            "[5/8, 2800/5000] loss: 1.632\n",
            "[5/8, 2900/5000] loss: 1.690\n",
            "[5/8, 3000/5000] loss: 1.745\n",
            "[5/8, 3100/5000] loss: 1.801\n",
            "[5/8, 3200/5000] loss: 1.858\n",
            "[5/8, 3300/5000] loss: 1.916\n",
            "[5/8, 3400/5000] loss: 1.972\n",
            "[5/8, 3500/5000] loss: 2.030\n",
            "[5/8, 3600/5000] loss: 2.091\n",
            "[5/8, 3700/5000] loss: 2.150\n",
            "[5/8, 3800/5000] loss: 2.208\n",
            "[5/8, 3900/5000] loss: 2.269\n",
            "[5/8, 4000/5000] loss: 2.330\n",
            "[5/8, 4100/5000] loss: 2.389\n",
            "[5/8, 4200/5000] loss: 2.449\n",
            "[5/8, 4300/5000] loss: 2.509\n",
            "[5/8, 4400/5000] loss: 2.568\n",
            "[5/8, 4500/5000] loss: 2.627\n",
            "[5/8, 4600/5000] loss: 2.686\n",
            "[5/8, 4700/5000] loss: 2.744\n",
            "[5/8, 4800/5000] loss: 2.802\n",
            "[5/8, 4900/5000] loss: 2.860\n",
            "[5/8, 5000/5000] loss: 2.916\n",
            "[6/8, 100/5000] loss: 0.053\n",
            "[6/8, 200/5000] loss: 0.112\n",
            "[6/8, 300/5000] loss: 0.166\n",
            "[6/8, 400/5000] loss: 0.220\n",
            "[6/8, 500/5000] loss: 0.273\n",
            "[6/8, 600/5000] loss: 0.328\n",
            "[6/8, 700/5000] loss: 0.381\n",
            "[6/8, 800/5000] loss: 0.438\n",
            "[6/8, 900/5000] loss: 0.495\n",
            "[6/8, 1000/5000] loss: 0.547\n",
            "[6/8, 1100/5000] loss: 0.602\n",
            "[6/8, 1200/5000] loss: 0.659\n",
            "[6/8, 1300/5000] loss: 0.713\n",
            "[6/8, 1400/5000] loss: 0.770\n",
            "[6/8, 1500/5000] loss: 0.827\n",
            "[6/8, 1600/5000] loss: 0.882\n",
            "[6/8, 1700/5000] loss: 0.938\n",
            "[6/8, 1800/5000] loss: 0.992\n",
            "[6/8, 1900/5000] loss: 1.048\n",
            "[6/8, 2000/5000] loss: 1.103\n",
            "[6/8, 2100/5000] loss: 1.162\n",
            "[6/8, 2200/5000] loss: 1.216\n",
            "[6/8, 2300/5000] loss: 1.275\n",
            "[6/8, 2400/5000] loss: 1.330\n",
            "[6/8, 2500/5000] loss: 1.385\n",
            "[6/8, 2600/5000] loss: 1.441\n",
            "[6/8, 2700/5000] loss: 1.495\n",
            "[6/8, 2800/5000] loss: 1.551\n",
            "[6/8, 2900/5000] loss: 1.606\n",
            "[6/8, 3000/5000] loss: 1.663\n",
            "[6/8, 3100/5000] loss: 1.715\n",
            "[6/8, 3200/5000] loss: 1.769\n",
            "[6/8, 3300/5000] loss: 1.828\n",
            "[6/8, 3400/5000] loss: 1.884\n",
            "[6/8, 3500/5000] loss: 1.937\n",
            "[6/8, 3600/5000] loss: 1.993\n",
            "[6/8, 3700/5000] loss: 2.047\n",
            "[6/8, 3800/5000] loss: 2.102\n",
            "[6/8, 3900/5000] loss: 2.157\n",
            "[6/8, 4000/5000] loss: 2.210\n",
            "[6/8, 4100/5000] loss: 2.264\n",
            "[6/8, 4200/5000] loss: 2.316\n",
            "[6/8, 4300/5000] loss: 2.373\n",
            "[6/8, 4400/5000] loss: 2.430\n",
            "[6/8, 4500/5000] loss: 2.483\n",
            "[6/8, 4600/5000] loss: 2.538\n",
            "[6/8, 4700/5000] loss: 2.594\n",
            "[6/8, 4800/5000] loss: 2.653\n",
            "[6/8, 4900/5000] loss: 2.708\n",
            "[6/8, 5000/5000] loss: 2.764\n",
            "[7/8, 100/5000] loss: 0.056\n",
            "[7/8, 200/5000] loss: 0.105\n",
            "[7/8, 300/5000] loss: 0.156\n",
            "[7/8, 400/5000] loss: 0.205\n",
            "[7/8, 500/5000] loss: 0.259\n",
            "[7/8, 600/5000] loss: 0.307\n",
            "[7/8, 700/5000] loss: 0.358\n",
            "[7/8, 800/5000] loss: 0.407\n",
            "[7/8, 900/5000] loss: 0.463\n",
            "[7/8, 1000/5000] loss: 0.513\n",
            "[7/8, 1100/5000] loss: 0.565\n",
            "[7/8, 1200/5000] loss: 0.618\n",
            "[7/8, 1300/5000] loss: 0.670\n",
            "[7/8, 1400/5000] loss: 0.723\n",
            "[7/8, 1500/5000] loss: 0.774\n",
            "[7/8, 1600/5000] loss: 0.827\n",
            "[7/8, 1700/5000] loss: 0.879\n",
            "[7/8, 1800/5000] loss: 0.932\n",
            "[7/8, 1900/5000] loss: 0.985\n",
            "[7/8, 2000/5000] loss: 1.035\n",
            "[7/8, 2100/5000] loss: 1.085\n",
            "[7/8, 2200/5000] loss: 1.138\n",
            "[7/8, 2300/5000] loss: 1.193\n",
            "[7/8, 2400/5000] loss: 1.246\n",
            "[7/8, 2500/5000] loss: 1.300\n",
            "[7/8, 2600/5000] loss: 1.353\n",
            "[7/8, 2700/5000] loss: 1.409\n",
            "[7/8, 2800/5000] loss: 1.457\n",
            "[7/8, 2900/5000] loss: 1.511\n",
            "[7/8, 3000/5000] loss: 1.560\n",
            "[7/8, 3100/5000] loss: 1.613\n",
            "[7/8, 3200/5000] loss: 1.668\n",
            "[7/8, 3300/5000] loss: 1.720\n",
            "[7/8, 3400/5000] loss: 1.772\n",
            "[7/8, 3500/5000] loss: 1.829\n",
            "[7/8, 3600/5000] loss: 1.879\n",
            "[7/8, 3700/5000] loss: 1.933\n",
            "[7/8, 3800/5000] loss: 1.985\n",
            "[7/8, 3900/5000] loss: 2.038\n",
            "[7/8, 4000/5000] loss: 2.091\n",
            "[7/8, 4100/5000] loss: 2.147\n",
            "[7/8, 4200/5000] loss: 2.201\n",
            "[7/8, 4300/5000] loss: 2.254\n",
            "[7/8, 4400/5000] loss: 2.310\n",
            "[7/8, 4500/5000] loss: 2.360\n",
            "[7/8, 4600/5000] loss: 2.412\n",
            "[7/8, 4700/5000] loss: 2.460\n",
            "[7/8, 4800/5000] loss: 2.516\n",
            "[7/8, 4900/5000] loss: 2.568\n",
            "[7/8, 5000/5000] loss: 2.621\n",
            "[8/8, 100/5000] loss: 0.049\n",
            "[8/8, 200/5000] loss: 0.102\n",
            "[8/8, 300/5000] loss: 0.149\n",
            "[8/8, 400/5000] loss: 0.197\n",
            "[8/8, 500/5000] loss: 0.245\n",
            "[8/8, 600/5000] loss: 0.294\n",
            "[8/8, 700/5000] loss: 0.341\n",
            "[8/8, 800/5000] loss: 0.390\n",
            "[8/8, 900/5000] loss: 0.438\n",
            "[8/8, 1000/5000] loss: 0.488\n",
            "[8/8, 1100/5000] loss: 0.538\n",
            "[8/8, 1200/5000] loss: 0.588\n",
            "[8/8, 1300/5000] loss: 0.638\n",
            "[8/8, 1400/5000] loss: 0.687\n",
            "[8/8, 1500/5000] loss: 0.738\n",
            "[8/8, 1600/5000] loss: 0.787\n",
            "[8/8, 1700/5000] loss: 0.840\n",
            "[8/8, 1800/5000] loss: 0.889\n",
            "[8/8, 1900/5000] loss: 0.938\n",
            "[8/8, 2000/5000] loss: 0.988\n",
            "[8/8, 2100/5000] loss: 1.037\n",
            "[8/8, 2200/5000] loss: 1.089\n",
            "[8/8, 2300/5000] loss: 1.140\n",
            "[8/8, 2400/5000] loss: 1.193\n",
            "[8/8, 2500/5000] loss: 1.239\n",
            "[8/8, 2600/5000] loss: 1.289\n",
            "[8/8, 2700/5000] loss: 1.339\n",
            "[8/8, 2800/5000] loss: 1.396\n",
            "[8/8, 2900/5000] loss: 1.448\n",
            "[8/8, 3000/5000] loss: 1.494\n",
            "[8/8, 3100/5000] loss: 1.547\n",
            "[8/8, 3200/5000] loss: 1.597\n",
            "[8/8, 3300/5000] loss: 1.644\n",
            "[8/8, 3400/5000] loss: 1.698\n",
            "[8/8, 3500/5000] loss: 1.744\n",
            "[8/8, 3600/5000] loss: 1.797\n",
            "[8/8, 3700/5000] loss: 1.844\n",
            "[8/8, 3800/5000] loss: 1.894\n",
            "[8/8, 3900/5000] loss: 1.949\n",
            "[8/8, 4000/5000] loss: 1.998\n",
            "[8/8, 4100/5000] loss: 2.049\n",
            "[8/8, 4200/5000] loss: 2.097\n",
            "[8/8, 4300/5000] loss: 2.151\n",
            "[8/8, 4400/5000] loss: 2.203\n",
            "[8/8, 4500/5000] loss: 2.250\n",
            "[8/8, 4600/5000] loss: 2.297\n",
            "[8/8, 4700/5000] loss: 2.345\n",
            "[8/8, 4800/5000] loss: 2.395\n",
            "[8/8, 4900/5000] loss: 2.449\n",
            "[8/8, 5000/5000] loss: 2.501\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wii4RQwMwdHL",
        "outputId": "9f24d141-014e-4e57-8e11-e79464385975"
      },
      "source": [
        "# validation accuracy\n",
        "now_valid_accuracy,now_valid_loss=cal_accuracy(net,testLoader)\n",
        "print('Accuracy of the network on the 10000 validation inputs: %.2f %%' % (now_valid_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test inputs: 87.52 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "3_fy9QzRzigV",
        "outputId": "3eddbb7e-7f77-4e80-e7e1-730331d6c920"
      },
      "source": [
        "#loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.array(train_loss_value), 'blue', label='train')\n",
        "plt.plot(np.array(valid_loss_value), 'r', label='validation')\n",
        "plt.legend()\n",
        "plt.show\n",
        "plt.savefig('/content/drive/MyDrive/VSAT_HW1/loss.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHBWO90kHcqQ"
      },
      "source": [
        "# Save model\n",
        "model_name='/content/drive/MyDrive/VSAT_HW1/cnn_model.pth'\n",
        "torch.save(net,model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrjYdUdMHBaC",
        "outputId": "107c1349-59ce-4088-e1e9-dd8b220c29df"
      },
      "source": [
        "# Load model\n",
        "net=torch.load('/content/drive/MyDrive/VSAT_HW1/cnn_model.pth')\n",
        "now_valid_accuracy,now_valid_loss=cal_accuracy(net,testLoader)\n",
        "print('Accuracy of the network on the 10000 validation inputs: %.2f %%' % (now_valid_accuracy))\n",
        "print('Top-3 error rate of the network on the 10000 validation inputs: %.2f %%' % (100-now_valid_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 validation inputs: 88.43 %\n",
            "Top-3 error rate of the network on the 10000 validation inputs: 11.57 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vccWsj6mHFDP"
      },
      "source": [
        "# test data\n",
        "x_test=np.load('/content/drive/MyDrive/VSAT_HW1/x_test.npy')\n",
        "x_test=convert_CHW(x_test)\n",
        "testset=MyDataset(data=x_test,label=y_valid,transform=None)\n",
        "testLoader = DataLoader(dataset=testset,batch_size=1,num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bIqYHllyKdK"
      },
      "source": [
        "# output\n",
        "ans=[]\n",
        "with torch.no_grad():\n",
        "    for data in testLoader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = net(inputs)\n",
        "        values, predicted = outputs.topk(3, dim=1, largest=True, sorted=True)\n",
        "        for i,data in enumerate(predicted,0):\n",
        "          ans.append(data.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFLJ7oLy0Ynx"
      },
      "source": [
        "# write\n",
        "txtpath='/content/drive/MyDrive/VSAT_HW1/0712534.txt'\n",
        "f = open(txtpath, 'w')\n",
        "for item in ans:\n",
        "  f.write(str(item)+'\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmeVdNDf1ksJ",
        "outputId": "4ee821ae-155f-4b5e-cdf5-525bd37817d3"
      },
      "source": [
        "# number of parameters\n",
        "number_of_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "print('The number of parameters: %d' %number_of_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of parameters: 62006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReNB585N3bhx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}